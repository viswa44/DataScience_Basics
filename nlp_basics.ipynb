{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "5ORwdJMAlswT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXAzElQ4lKST"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"Sisters and Brothers of America,\n",
        "\n",
        "It fills my heart with joy unspeakable to rise in response to the warm and cordial welcome which you have given us. I thank you in the name of the most ancient order of monks in the world, I thank you in the name of the mother of religions, and I thank you in the name of millions and millions of Hindu people of all classes and sects.\n",
        "\n",
        "My thanks, also, to some of the speakers on this platform who, referring to the delegates from the Orient, have told you that these men from far-off nations may well claim the honor of bearing to different lands the idea of toleration. I am proud to belong to a religion which has taught the world both tolerance and universal acceptance. We believe not only in universal toleration, but we accept all religions as true. I am proud to belong to a nation which has sheltered the persecuted and the refugees of all religions and all nations of the earth. I am proud to tell you that we have gathered in our bosom the purest remnant of the Israelites, who came to Southern India and took refuge with us in the very year in which their holy temple was shat­tered to pieces by Roman tyranny. I am proud to belong to the religion which has sheltered and is still fostering the remnant of the grand Zoroastrian nation. I will quote to you, brethren, a few lines from a hymn which I remember to have repeated from my earliest boyhood, which is every day repeated by millions of human beings: “As the different streams having their sources in different paths which men take through different tendencies, various though they appear, crooked or straight, all lead to Thee.”\n",
        "\n",
        "The present convention, which is one of the most august assemblies ever held, is in itself a vindication, a declaration to the world of the wonderful doctrine preached in the Gita: “Whosoever comes to Me, through whatsoever form, I reach him; all men are struggling through paths which in the end lead to me.” Sectarianism, bigotry, and its horrible descen­dant, fanaticism, have long possessed this beautiful earth. They have filled the earth with vio­lence, drenched it often and often with human blood, destroyed civilization and sent whole nations to despair. Had it not been for these horrible demons, human society would be far more advanced than it is now. But their time is come; and I fervently hope that the bell that tolled this morning in honor of this convention may be the death-knell of all fanaticism, of all persecutions with the sword or with the pen, and of all uncharitable feelings between persons wending their way to the same goal.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "id": "K2snDHORmSMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "Ai2sfuBxmiA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "09kP2v6vmZYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = sent_tokenize(corpus)###storing sentence in document variable"
      ],
      "metadata": {
        "id": "vJ71n3WUmuo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word tokenize"
      ],
      "metadata": {
        "id": "DRufEx7on_y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "ZUmBZVa3nalO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in documents:\n",
        "  print(word_tokenize(i))"
      ],
      "metadata": {
        "id": "IXKoWn6loMLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "token = TreebankWordTokenizer()  # Create an instance of the tokenizer\n",
        "tokenized_corpus = token.tokenize(corpus)  # Call tokenize on the instance, passing the corpus\n",
        "print(tokenized_corpus)  # Print the tokenized output"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "uaXQ_A5GqIEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##understand difference between different word tokenizations\n",
        "1.word tokenize - considers . as separate word\n",
        "2.treebank tokenize - consider \"word.\" as word\n",
        "and last . as separate word.\n"
      ],
      "metadata": {
        "id": "L83xTpgxqm9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "GE808_R0qiCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "is a process of reducing a word to a word stem"
      ],
      "metadata": {
        "id": "SWlEWo-ht098"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"eating\",\"eaten\",\"eats\",\"eat\",\"playing\",\"played\",\"plays\",\"playable\",\"cooking\",\"cooked\",\"cooks\",\"cooker\"]"
      ],
      "metadata": {
        "id": "tuP6LlbJtnzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Porterstemmer"
      ],
      "metadata": {
        "id": "uttFUcVvvQxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemming = PorterStemmer()"
      ],
      "metadata": {
        "id": "vX4wqPFyvNdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(stemming.stem(word))"
      ],
      "metadata": {
        "id": "u-hE0Sw_vcKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regexpstemmer - basically is cut downs ing , s ,se, able\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "nVQeX28YwZ49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)"
      ],
      "metadata": {
        "id": "hONqMyNUwhIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem('eating')"
      ],
      "metadata": {
        "id": "coFLl7Gtw2N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SnowballStemmer"
      ],
      "metadata": {
        "id": "-Ol_pXjhxRkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "sstemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "xzI7dfn6xQdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word, \"+ -----> \",sstemmer.stem(word))"
      ],
      "metadata": {
        "id": "2g50ubw_xhk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatizer"
      ],
      "metadata": {
        "id": "iozXLh3v0H4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('cooking',pos='v')\n",
        "'''\n",
        "pos = part of speech\n",
        "noun -n\n",
        "verb - v\n",
        "adverb - r\n",
        "adjective - a\n",
        "'''"
      ],
      "metadata": {
        "id": "dSWC_z8a0Kj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('cooking',pos='n')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8Xuv821N-5l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word, \"+ -----> \",lemmatizer.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "id": "JK1ZzBVu_TvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "wordnet lemmatization is called lemma\n",
        "wrapper around wordnet corpus\n"
      ],
      "metadata": {
        "id": "BN4CKnkLxhFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop words"
      ],
      "metadata": {
        "id": "P2II9QOZAkoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "import re"
      ],
      "metadata": {
        "id": "epXt8CKIAo9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "id": "3GsN31zgBUq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('french')"
      ],
      "metadata": {
        "id": "189OYSFSBbe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentence tokenization"
      ],
      "metadata": {
        "id": "JDVBVvvR4nA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "o_kgWOPx5N1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_of_corpus = nltk.sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "yY3vM7Lb6A_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "filter out stop words and thenn apply stemming"
      ],
      "metadata": {
        "id": "M7fo3Wo36WLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = []\n",
        "for i in range(len(sentences_of_corpus)):\n",
        "  WordNetCorpusReader = nltk.word_tokenize(sentences_of_corpus[i])\n",
        "  worded = [stemmer.stem(word) for word in WordNetCorpusReader if word not in set(stopwords.words('english'))]\n",
        "  list.append (' '.join(worded))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V5auXUIF6cHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowballstemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "M5-T64NhAQ3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowbalist = []\n",
        "for i in range(len(sentences_of_corpus)):\n",
        "  token_sentences=nltk.word_tokenize(sentences_of_corpus[i])\n",
        "  snow_stem_word = [snowballstemmer.stem(word) for word in token_sentences if word not in set(stopwords.words('english'))]\n",
        "  snowbalist.append(' '.join(snow_stem_word))"
      ],
      "metadata": {
        "id": "lKxnRU3C_Rt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list)\n",
        "print(snowbalist)"
      ],
      "metadata": {
        "id": "STH1Q9M2BhWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "RePly0jh6o3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llist = []\n",
        "for i in range(len(sentences_of_corpus)):\n",
        "  words = nltk.word_tokenize(sentences_of_corpus[i])\n",
        "  words = [lemmatizer.lemmatize(word,pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
        "  llist.append(' '.join(words))"
      ],
      "metadata": {
        "id": "LrKkfobK7CSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llist)"
      ],
      "metadata": {
        "id": "6Cg18qqr8SaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# parts of speech"
      ],
      "metadata": {
        "id": "oKI36bRw9ydj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "for i in range(len(sentences_of_corpus)):\n",
        "  words = nltk.word_tokenize(sentences_of_corpus[i])\n",
        "  words =[word for word in words if word not in set(stopwords.words('english'))]\n",
        "  pos_word = nltk.pos_tag(words)\n",
        "  print(pos_word)"
      ],
      "metadata": {
        "id": "C__cjvfN95kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition"
      ],
      "metadata": {
        "id": "nPXmdKiYBEan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "PM1d5k5ABI8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences_of_corpus)):\n",
        "  word = nltk.word_tokenize(sentences_of_corpus[i])\n",
        "  tag_ele = nltk.pos_tag(word)\n",
        "  named_entity = nltk.ne_chunk(tag_ele)\n",
        "  print(type(named_entity))\n",
        "  named_entity.draw()\n",
        ""
      ],
      "metadata": {
        "id": "bRymO7DkCGx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Chhatrapati Shivaji Maharaj was a Maratha warrior and founding ruler of the Maratha empire in western India. In India and even in other countries, he is still considered the greatest warrior of his time. An innovative military tactician and a skilful administrator, he is considered a valorous warrior.\""
      ],
      "metadata": {
        "id": "4k_chVL3DWSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_chatrapathi = nltk.word_tokenize(sentence)\n",
        "print(words_chatrapathi)"
      ],
      "metadata": {
        "id": "4e28LihBE7Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_elements = nltk.pos_tag(words_chatrapathi)\n",
        "print(tag_elements)"
      ],
      "metadata": {
        "id": "rzLSnmHfFH79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling"
      ],
      "metadata": {
        "id": "GcRhLDZkFZ06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import svgling\n",
        "nltk.ne_chunk(tag_elements)"
      ],
      "metadata": {
        "id": "az5nJJgCFRBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding"
      ],
      "metadata": {
        "id": "bzakAPXwOAv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding - is not used for nlp\n",
        "#we use bag of words, word2vec(dl), tfidf\n",
        "#cbow,skipgram ---> word2vec,avgword2vec\n",
        "#word2vec uses neural network model to learn word associationa from large corpus of text.\n",
        "#it can suggest words to sentence"
      ],
      "metadata": {
        "id": "xek7G3j3OIGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "hJg3fnvzHNyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "E2lxdE5eHVpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n"
      ],
      "metadata": {
        "id": "llhmEk1jHRjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "a0G6EJsUHpvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv_king=wv['good']"
      ],
      "metadata": {
        "id": "xuJp-JOuLk5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv_king"
      ],
      "metadata": {
        "id": "kVdYQlYfLwFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv.most_similar('good')"
      ],
      "metadata": {
        "id": "Q0Sy6GulMEMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = wv['king'] - wv['man'] + wv['woman']"
      ],
      "metadata": {
        "id": "OQb3pFv0M19x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv.most_similar([vec])"
      ],
      "metadata": {
        "id": "Gx_jgIC5M4Hw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}