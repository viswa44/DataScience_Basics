{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP - DL"
      ],
      "metadata": {
        "id": "nuKuUthqlxs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is called n-gram models?\n",
        "# These models predict next word based on previous n-1 words.\n",
        "# they require large amount of data to predict sequence of word.\n",
        "# they suffer from ccurse dimensonality problem."
      ],
      "metadata": {
        "id": "CUzckV231lrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what is nnlm?\n",
        "# NNLM were proposed to overcome limitations of n-gram models.\n",
        "# key idea is used neural networks to capture context of sequence of words."
      ],
      "metadata": {
        "id": "c_le7Zj16XhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##components of nnlm\n",
        "#word embeddings - represented as dense vectors in continous vector space.\n",
        "#nn - i/p - word embeddings\n",
        "# hidden layer - transform embeddings to higher form - activation function\n",
        "# o/p - softmax layer that produces probability distribution over vocabulary."
      ],
      "metadata": {
        "id": "ftALMvUZ7nYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Advantages of NNLM\n",
        "# Generalization: NNLMs generalize better to unseen word sequences due to the continuous representation of words.\n",
        "# Contextual Understanding: They can capture more complex patterns and dependencies in the data.\n",
        "# Performance: NNLMs have shown to perform better than traditional n-gram models in various NLP tasks."
      ],
      "metadata": {
        "id": "0iW1ka3P_GxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applications\n",
        "# Text Generation: NNLMs are used to generate coherent and contextually relevant text.\n",
        "# Machine Translation: They improve the quality of translations by providing better context understanding.\n",
        "# Speech Recognition: NNLMs enhance the accuracy of recognizing spoken words by considering the context."
      ],
      "metadata": {
        "id": "fJpUNp01Ax9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word embeddings -- representation of words in vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "-tHrgmuEHhQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #word2vec -- built on  dl neural network -- nn to learn word associations from large corpus\n",
        "# - cbow\n",
        "# -skipgram\n"
      ],
      "metadata": {
        "id": "YHm1oakEINeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature representation(imp)\n",
        "#cosine similarity - cos 45\n",
        "# distance = 1- cos similarity\n"
      ],
      "metadata": {
        "id": "k_j-pQS8I2Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # cbow -- is rnn\n",
        "# # window size = 5\n",
        "# # ip - 4 op - 1\n",
        "# # skipgram -- is cnn\n",
        "# # ip = 1\n",
        "# # op = 4\n",
        "# small corpus = cbow\n",
        "# large corpus = skipgram"
      ],
      "metadata": {
        "id": "W1uMSlDcKEKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#glove  -  GloVe constructs a co-occurrence matrix of the words in the corpus and then factorizes this matrix to obtain word vector\n",
        "# Users can select a pre-trained GloVe embedding in a dimension (e.g., 50-d, 100-d, 200-d, or 300-d vectors) that best fits their needs in terms of computational resources and task specificity.\n",
        "# text classification - ner - translation - question and answers - document similarity -"
      ],
      "metadata": {
        "id": "3ffdAMvYMuOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Glove model is based on leveraging global word to word co-occurance counts leveraging the entire corpus. Word2vec on the other hand leverages co-occurance within local context (neighbouring words)."
      ],
      "metadata": {
        "id": "IHipUX5PhH4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lstm - short term memory, long term memory, 3 gates - forget,ip,op gate\n",
        "#gru - mixed for short memory and long memory, 2 gates - update gate, reset gate"
      ],
      "metadata": {
        "id": "h8pyJcboG53P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert is built on transformer architecture\n",
        "# lets understand transformer architecture\n",
        "# positional encodings -- >Positional encoding helps the transformer model understand the order of tokens in the input sequence.\n",
        "# It provides a way to inject some information about the positions into the model since\n",
        "# transformers process tokens in parallel rather than sequentially.(*)"
      ],
      "metadata": {
        "id": "cLIpp70-Y9ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation:\n",
        "\n",
        "# Positional encodings are usually implemented as an additional matrix that is added to the input embedding matrix.\n",
        "# In practice, this matrix is precomputed and added to the input embeddings during training and inference."
      ],
      "metadata": {
        "id": "P6XqZR-VfOBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention mechanism\n",
        "#is a technique that allows models to focus on specific parts of the input sequence\n",
        "#when producing each element of the output sequence.\n",
        "#  It assigns different weights to different input elements,\n",
        "#  enabling the model to prioritize certain information over others.\n",
        "#  This is particularly useful in tasks like language translation,\n",
        "#  where the meaning of a word often depends on its context."
      ],
      "metadata": {
        "id": "Ab4lnkg0huu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#types of mechanism in transformers\n",
        "#scaled dot - product attention --> attention(q,k,v) = softmax(qkT/sq(dk))V  --> dk dimension of k vectors\n",
        "#multi head attention --> Multi-Head Attention enhances the model’s ability to focus on different parts of the input sequence simultaneously. It involves multiple attention heads, each with its own set of query, key, and value matrices. The outputs of these heads are concatenated and linearly transformed to produce the final output. This allows the model to capture different features and dependencies in the input sequence.\n",
        "#self attention -->  In the context of the Transformer, self-attention is applied in both the encoder and decoder layers. It enables the model to capture long-range dependencies and relationships within the input sequence.\n",
        "#encoder-decoder attention --> also known as cross-attention,\n",
        "# is used in the decoder layers of the Transformer.\n",
        "#  It allows the decoder to focus on relevant parts of the input sequence (encoded by the encoder)\n",
        "#   when generating each word of the output sequence.\n",
        "#    This type of attention ensures that the decoder has access to the entire input sequence,\n",
        "#     helping it produce more accurate and contextually appropriate translations.\n",
        "#casual / masked self attention --> Masked Self-Attention is used in the decoder to ensure that the prediction for a given position only depends on the known outputs at positions before it.\n",
        "#  This is crucial for tasks like language modeling,\n",
        "#   where future tokens should not be visible during training.\n",
        "#    The attention scores for future tokens are masked out, ensuring that the model cannot look ahead."
      ],
      "metadata": {
        "id": "rsIOzdy8iutU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #bert\n",
        "# In the pre-BERT world,\n",
        "# a language model would have looked at this text sequence during training from either left-to-right or combined left-to-right and right-to-left.\n",
        "# This one-directional approach works well for generating sentences —\n",
        "# we can predict the next word, append that to the sequence, then predict the next to next word until we have a complete sentence.\n",
        "\n",
        "# Now enters BERT, a language model which is bidirectionally trained (this is also its key technical innovation).\n",
        "# This means we can now have a deeper sense of language context and flow compared to the single-direction language models.\n",
        "# # Instead of predicting the next word in a sequence,\n",
        "# BERT makes use of a novel technique called Masked LM (MLM): it randomly masks words in the sentence and then it tries to predict them.\n",
        "#  Masking means that the model looks in both directions and it uses the full context of the sentence, both left and right surroundings,\n",
        "#  in order to predict the masked word. Unlike the previous language models, it takes both the previous and next tokens into account at the same time.\n",
        "#  The existing combined left-to-right and right-to-left LSTM based models were missing this “same-time part”.\n",
        "#   (It might be more accurate to say that BERT is non-directional though.)"
      ],
      "metadata": {
        "id": "9BD7RPAOn4OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Significance of Transformer Attention Mechanism\n",
        "# The attention mechanism in Transformers offers several advantages:\n",
        "\n",
        "# Parallel Processing: Unlike RNNs, Transformers can process all words in a sequence simultaneously, significantly reducing training time.\n",
        "# Long-Range Dependencies: The attention mechanism can capture relationships between distant words, addressing the limitations of traditional models that struggle with long-range dependencies.\n",
        "# Scalability: Transformers can handle larger datasets and complex tasks due to their scalable architecture."
      ],
      "metadata": {
        "id": "On_aMt3krvZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applications\n",
        "# translations\n",
        "# text summarizations\n",
        "# q & a\n",
        "# sentiment analysis"
      ],
      "metadata": {
        "id": "6CZa8t6sstjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There will need to be token embeddings to mark the beginning and end of sentences. You'll need to have segment embeddings to be able to distinguish different sentences. Lastly you'll need positional embeddings to indicate the position of words in a sentence.\n",
        "[CLS] the [MASK] has blue spots [SEP] it rolls [MASK] the parking lot [SEP]\n",
        "Once it's finished predicting words, then BERT takes advantage of next sentence prediction. This looks at the relationship between two sentences. It does this to better understand the context of the entire data set by taking a pair of sentences and predicting if the second sentence is the next sentence based on the original text."
      ],
      "metadata": {
        "id": "l3sScKNJyMCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "For next sentence prediction to work in the BERT technique, the second sentence is sent through the Transformer based model.\n",
        "\n",
        "The drawback to this approach is that the loss function only considers the masked word predictions and not the predictions of the others. That means the BERT technique converges slower than the other right-to-left or left-to-right techniques.\n",
        "\n"
      ],
      "metadata": {
        "id": "m1WB3O_7zCdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "937rTtHTs69H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert is transformer based model architecture.\n",
        "# Transformer (the attention mechanism that learns contextual relationships between words in a text)"
      ],
      "metadata": {
        "id": "0CTrlmVFrbSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gpt - genarative pre-trained transformer\n",
        "#auto regressive language model - 45TB\n",
        "So, GPT-3 excels in language modeling for tasks like text generation, while BERT's pre-training method focuses on understanding natural language through masked language modeling.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MwY6BF6yG8cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#huggingface library\n",
        "#The Hugging Face Transformer Library is an open-source library that provides a vast array of pre-trained models primarily focused on NLP. It’s built on PyTorch and TensorFlow, making it incredibly versatile and powerful.\n",
        "directly can access pre trained model through hugging face library\n"
      ],
      "metadata": {
        "id": "ScLaPFyIGbuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning Neural Network"
      ],
      "metadata": {
        "id": "OOuYFksil1dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#grid search\n",
        "# random search\n",
        "#baysean optimization\n",
        "\n",
        "# Advantages of Hyperparameter tuning:\n",
        "# Improved model performance\n",
        "# Reduced overfitting and underfitting\n",
        "# Enhanced model generalizability\n",
        "# Optimized resource utilization\n",
        "# Improved model interpretability\n",
        "\n",
        "\n",
        "# Disadvantages of Hyperparameter tuning:\n",
        "# Computational cost\n",
        "# Time-consuming process\n",
        "# Risk of overfitting\n",
        "# No guarantee of optimal performance\n",
        "# Requires expertise"
      ],
      "metadata": {
        "id": "BfeGX5PnH_Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# layers | loss functions | custom metrics"
      ],
      "metadata": {
        "id": "-rFYdB__mCqe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l2p8BDwsKAzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# word2vec, glove, bert, gpt, encoder-decoder architecture, hugging face library\n"
      ],
      "metadata": {
        "id": "zp2c_aahmYc1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EJ2bzuJwKFjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  understanding of defensive coding practices.\n"
      ],
      "metadata": {
        "id": "q9NY5kTLmbJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input validation - use regex\n",
        "error handling - try catch | log errors\n",
        "boundary checks - validate indices of list and arrays and pointers\n",
        "Assertions - If your plaform have assertion mechanism you can use it to check for conditions that should always be true during development. You can enable assertions during testing and development and disable those in production.\n",
        "default values - Always provide default values for variables, especially when dealing with user inputs or external data. You should avoid relying on implicit assumptions about default behavior.\n",
        "secure coding practices - Apply secure coding practices to prevent vulnerabilities like SQL injection, cross-site scripting, etc. Use parameterized queries for database interactions.\n",
        "07 Code Reviews\n",
        "logging\n",
        "monitoring\n",
        "use immutable data structures\n",
        "unit testing\n",
        "fail- safe mechanism\n",
        "\n"
      ],
      "metadata": {
        "id": "UvcWTLrJHYbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# automating tasks to maximise time for more important problems.\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kq-LnPvlmwtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Visualization experience (via Grafana, Superset, Tableau,\n"
      ],
      "metadata": {
        "id": "2VTqubwum17g"
      }
    }
  ]
}