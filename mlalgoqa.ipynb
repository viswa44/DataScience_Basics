{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "QKveZKJBL-8n",
        "4i7UsYdW3qQ7",
        "aREQsSg8wX80",
        "5H9ifkBHw4wC",
        "8d7YIG7Ow9ae",
        "Ebyf131hxA0T",
        "SMNVL2u0xC-y",
        "NWWTK1Iexcs1",
        "Wjm1dDuFxg17",
        "asDMDI5sx30P"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "QKveZKJBL-8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#y = mx + c\n",
        "y - dependent variable\n",
        "x-  independent\n",
        "m - slope\n",
        "c - intercept"
      ],
      "metadata": {
        "id": "BxkAwHhoMMaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "how to define best fit line?\n",
        "OLS = ordinary least square\n"
      ],
      "metadata": {
        "id": "WD-s-Bj0Mhed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asssumptions\n",
        "realtionship btwn variables is linear\n",
        "outliers\n",
        "multicollinearity - how you can deal with it? --> pca --> regularization techniques\n",
        "homoscadisity\n",
        "normality"
      ],
      "metadata": {
        "id": "GvINd1m9M8rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance of linear regression\n",
        "mse -\n",
        "rmse -\n",
        "mae - mean absolute error\n",
        "r2 -\n",
        "adjusted r2\n",
        "\n"
      ],
      "metadata": {
        "id": "2SeQjXH4NSkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2 - how well data points are fitting in the model\n",
        "adj r2 -"
      ],
      "metadata": {
        "id": "rbh8Nx1cN-ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "why multi collinearity is problem?\n",
        "\n",
        "two or more independent variables are highly correlated with each other\n",
        "\n",
        "unstable coefficients\n",
        "inflated standard errors\n",
        "\n",
        "vif (varience inflation factor)--  can be used to detect multi collinearity\n"
      ],
      "metadata": {
        "id": "yFXnzUVAOYFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "how can you handle missing data?"
      ],
      "metadata": {
        "id": "A2EY3yGGPRAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputation - mode, mean, interpolation, random forest\n",
        "knn\n"
      ],
      "metadata": {
        "id": "7HHBAlRLO7Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear regression. vs logistic regression?\n",
        "\n",
        "continious values- regression\n",
        "categorical values - logistic regression\n",
        "\n",
        "logistic regression uses \"s\" shaped curve"
      ],
      "metadata": {
        "id": "Ww1E692APU4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if linear regression captures too much of data including noise outliers is called over fitting"
      ],
      "metadata": {
        "id": "fXyLanLVP7v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Batch gradient decent -\n",
        "\n",
        "uses entire dataset\n",
        "stable and consistent\n",
        "\n",
        "\n",
        "stochastic gradient decent -\n",
        "uses single data point at a time\n"
      ],
      "metadata": {
        "id": "wu0XU6eEQiCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "why regularization?\n",
        "prevent overfitting\n",
        "\n",
        "ridge l2 -\n",
        "lasso l1 -\n",
        "elastic net -"
      ],
      "metadata": {
        "id": "aNKNWS-FRVfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1 vs l2\n",
        "l1 - eliminates less important features\n",
        "it sets some coeffients to zero\n",
        "l2 - shrinks the coefficients but doesnot eliminate"
      ],
      "metadata": {
        "id": "1Zpn68wER02i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "how does feature scaling effect ?\n",
        "\n",
        "- used for consistency of coefficients\n",
        "- minmaxscaling used for scaling features in a given range."
      ],
      "metadata": {
        "id": "-nI3xtELSMwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ols -- ols is the method used to estimate slope the minimises sum of residual squares(rss)"
      ],
      "metadata": {
        "id": "i9X13UlYsn03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias variance tradeoff - The bias-variance trade-off is about finding a balance between bias and\n",
        "variance to minimize the total prediction error. This trade-off can be visualized as follows."
      ],
      "metadata": {
        "id": "hnNossJMsq-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "why cross validation is important?\n",
        "cross validation - measures how well a model generalizes to unseen data\n",
        "k-fold cross validation"
      ],
      "metadata": {
        "id": "41dvylGAznJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning rate in gradient decent -\n",
        "learning rate is hyper parameter\n",
        "0 to 1\n",
        "size of step in gradient decent\n"
      ],
      "metadata": {
        "id": "qG-a0dJyzDVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polynomial regression\n",
        "pros:\n",
        "\n",
        "flexibility\n",
        "better fit for curve data\n",
        "higher accuracy for non linear data\n",
        "---------------\n",
        "cons:\n",
        "more sensitive to outliers\n",
        "careful handling\n"
      ],
      "metadata": {
        "id": "PiDc30a-05q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supervised learning\n",
        "-->\n",
        "unsupervised learning\n",
        "-->"
      ],
      "metadata": {
        "id": "gYMoZezp1igK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1-score\n",
        "1. Imbalanced Classes\n",
        "2. Medical Diagnostics\n",
        "3. Spam Detection\n",
        "4. Customer Support\n",
        "\n",
        "it is used when you need to balance the precision and recall of the model.\n",
        "f1 score is used in cases where to consider false positive cases also\n"
      ],
      "metadata": {
        "id": "ulrxpGiD19IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E1hYbTVv3cWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression"
      ],
      "metadata": {
        "id": "4i7UsYdW3qQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic regression is used for classification problems\n",
        "it estimates probability of element occuring in particular class.\n"
      ],
      "metadata": {
        "id": "NuzUrowa3uPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid fn -\n",
        "or logistic function\n",
        "\n",
        "f(x) = 1/(1+e^-x)\n",
        "input can be classified into two classes"
      ],
      "metadata": {
        "id": "g16Xga-j42rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost function in logistic regression\n",
        "-- measures how well the model fits the data\n",
        "-- log loss\n",
        "-- binary cross entropy\n",
        "-- A cost function measures the disparity between predicted values\n",
        "and actual values in a machine learning model. It quantifies how well the model aligns with the ground truth, guiding optimization.\n",
        "\n"
      ],
      "metadata": {
        "id": "hTwJwUwv5IQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "In logistic regression, the goal of the cost function is to measure how well the model’s\n",
        "predictions align with the actual outcomes, and to optimize the model parameters to improve this alignment. Here’s a detailed breakdown:\n",
        "\n"
      ],
      "metadata": {
        "id": "mrlY0HDa5dTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi collinearity causes\n",
        "unstable co efficients\n",
        "vif - variance inflation factor - detects\n"
      ],
      "metadata": {
        "id": "HnPudGaH6oew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpret co-efficients\n",
        "logarithm of odds\n",
        "odds ratio"
      ],
      "metadata": {
        "id": "V8pTRg-K7F7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics\n",
        "confusion matrix\n",
        "accuracy\n",
        "precision\n",
        "recall\n",
        "log loss\n",
        "roc curve\n",
        "auc\n",
        "binary cross entropy\n"
      ],
      "metadata": {
        "id": "LkzpePYZ8pyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regularization\n",
        "l1 - lasso\n",
        "l2 - ridge\n",
        "control size of co efficients"
      ],
      "metadata": {
        "id": "_SyMIuIQ8Vqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision boundary -\n",
        "set of points\n",
        "where the model’s predicted probability is exactly equal to the threshold.\n",
        "In binary classification, this corresponds to the point where the probability is 0.5."
      ],
      "metadata": {
        "id": "q4TRM9Ni9dKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "handling imbalance dataset\n",
        "resampling\n",
        "undersampling\n",
        "oversampling\n",
        "threshold moving\n",
        "bagging boosting\n",
        "data augmentation\n",
        "scaling"
      ],
      "metadata": {
        "id": "hwSb5lGp96c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROC curves in logistic regression are used for determining the \"best cutoff value\" for\n",
        "predicting whether a new observation is a \"failure\" (0) or a \"success\" (1)"
      ],
      "metadata": {
        "id": "gHfhheLA-f0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maximum likelihood function:\n",
        "\n",
        "estimate parameters\n",
        "Goodness of Fit: The log-likelihood value provides an indication of\n",
        "how well the model fits the data. Higher log-likelihood values indicate better fit.\n"
      ],
      "metadata": {
        "id": "eaEGrMCF_SZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assumptions:\n",
        "\n",
        "outliers influence decision boundary\n",
        "multicollinearity\n",
        "linearity\n",
        "residuals are independent of each other\n",
        "large sample size\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N6jVtA5mA098"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"corellation matrix\" detects multi collinearity"
      ],
      "metadata": {
        "id": "vuBAYtOFBl1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Understanding Impact:\n",
        "Odds ratios provide a clear understanding of\n",
        "how changes in \"predictor variables\" affect the odds of the \"outcome\", which can be more intuitive than interpreting raw coefficients.\n",
        "\n",
        "Comparing Effects: They allow for easy comparison of the effects of different predictor variables on the outcome."
      ],
      "metadata": {
        "id": "wSsk7ZytCT_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avoid underfitting:\n",
        "\n",
        "outliers influence decision boundary\n",
        "multicollinearity\n",
        "linearity\n",
        "residuals are independent of each other\n",
        "large sample size\n",
        "reduce regularizations"
      ],
      "metadata": {
        "id": "HZkyY-ghDa39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common challenges?\n",
        "\n",
        "linearity assumptions\n",
        "limited to binaary classifications\n",
        "outliers\n",
        "multicollinearity\n",
        "requires large sample size\n",
        "residuals are independent of each other\n"
      ],
      "metadata": {
        "id": "aF0ZgLm7EBoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p value in logistic regression\n",
        "determine significance of predictor variables\n"
      ],
      "metadata": {
        "id": "Y9Xjjx-4FAYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Precision:\n",
        " Precision measures the accuracy of positive predictions.\n",
        "It is the proportion of true positive predictions among all positive predictions made by the model.\n",
        "\n",
        "Recall: Recall (or Sensitivity)\n",
        "measures how well the model identifies positive instances.\n",
        " It is the proportion of true positive predictions among all actual positive instances.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l71oYvGRG8PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "how do you interpret coefficients in logistic regression\n",
        "\n",
        "1. Log-Odds and Coefficients\n",
        "2. Interpreting Coefficients\n",
        "3. Exponentiating Coefficients\n",
        "4. Interpreting the Odds Ratio\n"
      ],
      "metadata": {
        "id": "jX3wA3O6HZ61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Trees"
      ],
      "metadata": {
        "id": "aREQsSg8wX80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root node : The base of decision tree\n",
        "splitting node : processing of dividing nodes\n",
        "decision mode : it is a  sub node can be further split into additional sub-nodes.\n",
        "\n",
        "leaf node : sub-node is not splitting into further nodes.\n",
        "\n",
        "information gain\n",
        "entropy\n",
        "gini index\n",
        "\n",
        "pre pruning\n",
        "post pruning\n",
        "\n"
      ],
      "metadata": {
        "id": "6VyAWQX164j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "entropy?\n",
        "measures impurity of data\n",
        "higher the value high impure\n",
        "entropy uses logarithm fn to calculate\n",
        "\n",
        "\n",
        "gini impurity?\n",
        "alternative too entropy but efficient than entropy\n",
        "gini uses square of probability (simple maths )\n",
        "Information gain?\n",
        "\n",
        "used to measure best of features\n",
        "higher ig feature can be used for splitting\n"
      ],
      "metadata": {
        "id": "OLUjl_G3_u9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "advantages :\n",
        "\n",
        "interpretability\n",
        "no need of feature scaling\n",
        "handles both numerical and categorical data\n",
        "non-linear relationship\n",
        "little data preparation\n",
        "inherent feature selection\n",
        "\n",
        "disadvantages:\n",
        "overfitting\n",
        "instability\n",
        "bias in imbalanced dataset\n",
        "greedy algo\n",
        "complex with large data\n",
        "prone to outliers\n",
        "\n"
      ],
      "metadata": {
        "id": "53Y1cwdp-41H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avoid overfitting\n",
        "\n",
        "pre pruning\n",
        "post pruning\n",
        "\n",
        "setting maximum depth\n",
        "maximum samples per leaf\n",
        "\n",
        "random forest\n",
        "cross validation\n",
        "\n"
      ],
      "metadata": {
        "id": "dMgg-4qK_3Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision tree vs random forest\n",
        "dt is simple, interpretable but prone to overfitting\n",
        "and sensitive to data variations\n",
        "\n",
        "random forest\n",
        "complex less interpretable\n",
        "but more accurate stable resistant to overfitting"
      ],
      "metadata": {
        "id": "ymn8Us5CAEhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pruning\n",
        "pruning is a processs of cutting down less important nodes in a decision tree, which do not contribute significantly to the model's performance.\n"
      ],
      "metadata": {
        "id": "2AAU4QC3vQCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can decisiontree can handle missing values??\n",
        "three methods\n",
        "surrogate split - using alternative features when primary features are missing\n",
        "ignoring missing values  - simply skipping missing values during skipping\n",
        "imputation - replace it with mode\n"
      ],
      "metadata": {
        "id": "mNaasMSXc3Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. How does a Decision Tree handle features with different scales?\n",
        "\n",
        "distance metrics:\n",
        "euclidean distance\n",
        "hamming distance\n",
        "minkowski distance\n",
        "\n",
        "unlike other algos decison tree don not use decision metrics\n",
        "DT is not affected by scaling\n",
        "Dt uses gini index and entropy\n",
        "making them inherently robust to feature scaling."
      ],
      "metadata": {
        "id": "wvuv_ietfi-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "How do you handle categorical features in a Decision Tree?\n",
        "\n",
        "feature grouping -  similar categories togather to reduce number of features to one\n",
        "target encoding - replace categorical values with the mean of the target variable for that category.\n",
        "weight of evidence - convert categorical variables into continuous numerical variables\n",
        "hashing trick : Map categories to integer using hash function, which is useful for high cardinality feature in large dataset.\n"
      ],
      "metadata": {
        "id": "L4T_PuA0ipQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decison tree prone to overfitting, how does ensemble learning mitigate this?\n",
        "Ensemble Learning particularly techniques like random forest and gradient boosting can mitigate overfitting in decision trees.\n",
        "mitigates overfitting by,\n",
        "\n",
        "\n",
        "Bagging(Bootstrap Aggregating) - random forest create nultiple decision tree by sampling data with replacement and aggregating their predictions, which reduces variance.\n",
        "\n",
        "Boosting - gradient boosting machine build tree sequential with each tree correcting the errors of the previous one,which reduces bias while controlling variance.\n",
        "\n",
        "Feature Randomization - Random Forests randomly select a subset of feature for each tree, which prevents any one feature from dominating the model and reduces overfitting."
      ],
      "metadata": {
        "id": "HUohm3I48Tpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variance reduction in context of Decision tree:\n",
        "\n",
        "variance reduction is criterion often used in regression trees to decide\n",
        "the best splits.\n",
        "It measures how much splitting a node reduces the variance in the resulting child nodes.\n"
      ],
      "metadata": {
        "id": "soKu_EjpG21K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "handle bias-variance trade off in decision tree?\n",
        "pruning\n",
        "ensemble methods - random forest, gradient boosting reduces variance by averaging the predictions of multiple models\n",
        "hyperparameter tuning - adjust like tree depth, minimum samples per leaf, minimum samples to split\n"
      ],
      "metadata": {
        "id": "LxUqP02yJAuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision tree with imbalanced datasets\n",
        "\n",
        "\n",
        "decision tree can be biased towards the majority class in imbalanced datasets.\n",
        "because of splitting criteria\n",
        "\n",
        "class weighting - assign higher weights to minority class to makethe more sensitive.\n",
        "balanced simpling - use SMOTE or undersampling of majority class to create balanced dataset.\n",
        "cost sensitive learning - cost of misclassification directly into decision making process, where misclassifying the minority class has higher cost.\n"
      ],
      "metadata": {
        "id": "cUaJyXg7LYTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curse of dimensionality in the context of decision tree\n",
        "\n",
        "sparse data: Number of dimensions increases, data points become sparse, making it difficult for the tree to find optimal splits.\n",
        "overfitting\n",
        "computational complexity - complexity of finding best split grows with number of features\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJRig15DRxgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dimwnsionality reduction in decision tree\n",
        "\n",
        "pca\n",
        "LIMITING TREE DEPTH\n",
        "\n"
      ],
      "metadata": {
        "id": "-YBiYrRsbW0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INTERPRETABILITY OF DT?\n",
        "\n",
        "dt are easy to interpret\n",
        "VISUAL REPRESENTATION\n",
        "FEATURE IMPORTANCE -\n",
        "IMPORTANCE OF FEATURE CAN BE EASILY INFERRED BASED ON THEIR\n",
        "POSITIONS IN THE TREE AND REDUCTION IN IMPURITY\n"
      ],
      "metadata": {
        "id": "emeZuXvQdj4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURE IMPORTANCE IN DECISION TREE\n",
        "- Gini Impurity\n",
        "- Information gain\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a3YldBtFfeHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Feature Importance(f)=\n",
        "nodes where f is used\n",
        "∑\n",
        "​\n",
        "\n",
        "N\n",
        "total\n",
        "​\n",
        "\n",
        "N\n",
        "node\n",
        "​\n",
        "\n",
        "​\n",
        " ×ΔCriterion"
      ],
      "metadata": {
        "id": "dyVpv4zdhSEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11. Why might a Decision Tree algorithm perform poorly on a particular dataset, and how would you diagnose and fix the issue?\n"
      ],
      "metadata": {
        "id": "KLdPMYO-hm9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "5H9ifkBHw4wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Random forest is one of \"Bagging technique\"\n",
        "decision tree is part of random forest\n",
        "data --> row sampling + feature dampling --> DT --> Aggregation\n",
        "base learner - decision tree\n",
        "\n"
      ],
      "metadata": {
        "id": "jWZkq8XAbDRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "why we are using random forest ?\n",
        "as we know dt is low bias , high variance\n",
        "random forest high variance converts to low variance\n"
      ],
      "metadata": {
        "id": "m5ZcxGKsgGBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in regression problem we use mean median or mode.\n",
        "in classification problem we use majority vote."
      ],
      "metadata": {
        "id": "RFXCTGXVgVuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Advantages of random forest?\n",
        "reduces overfitting\n",
        "higher accuracy\n",
        "robustness to noisey data\n",
        "feature importance\n",
        "parallel processing"
      ],
      "metadata": {
        "id": "RZTUIz9vhUvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "how random forest handle missing values>\n",
        "in split handling - ignores missing values\n",
        "imputation - missing values replaced by most common value for classification\n",
        "mean for regression\n",
        "surrogate split - random forest can use surrogate split,\n",
        "where another feature that closely mimics the behaviour of primary split feature is used when the primary feature is missing.\n"
      ],
      "metadata": {
        "id": "KjRQahx3inBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature randomness refers to selecting  a random subset of features\n",
        "for each split in the decision tree.\n",
        "\n",
        "reduce correlation: by introducing randomness, that the trees in forest are not same, taht reduces correlation btwn the trees\n",
        "improve generalization: by de - correlating the modelis less likely to overfit\n",
        "to training data and more likely to genralize new data."
      ],
      "metadata": {
        "id": "gl6TGrtUlGOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "what is oob (out of bag) in random forest?\n",
        "metrics - oob\n",
        "oob is nothing but performance of the model on unseen data\n"
      ],
      "metadata": {
        "id": "L1uGAXPGnwNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "how do u determine optimal dt for rf?\n",
        "cross validation\n",
        "out of bag error"
      ],
      "metadata": {
        "id": "70Xcb4ngpgEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature importance\n",
        "information gain\n",
        "gini impurity\n"
      ],
      "metadata": {
        "id": "s9vPIorEqdBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "limitations of random forest??\n",
        "complexity\n",
        "interpretability - less transparent and hard to interpret\n",
        "can be biased towards majority class unless techniques like class weighting or balanced sampling are used."
      ],
      "metadata": {
        "id": "OmSSsPIssX4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree correlation in random forest??\n",
        "tree correlation referes to similarity in structure and decision.\n",
        "high correlation between trees mEANS tree are likely making similar errors\n",
        "which can lead to overfitting.\n",
        "\n",
        "random fores reduce tree correlation by row sampling and feature sampling.\n",
        "leading to more diverse trees and sronger ensemble model."
      ],
      "metadata": {
        "id": "mX3KJ2M7s1WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random forest vs gradient boost machines??\n",
        "gradient boosting machines an ensemble method builds tree sequentially,\n",
        "where each tree is trained to correct the errors of the previous tree.\n",
        "\n",
        "GBM typically provides better accuracy than rf at cost of increased training time and risk of overfitting."
      ],
      "metadata": {
        "id": "mUVKxcw-zn9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Hyperparameters:\n",
        "\n",
        "max_features :\n",
        "this parameter consider number of feature in eaach split\n",
        "sma;;er max_features increse in randomness it can reduce overfittting\n",
        "biiger max_feature reduces randomness improves accuracy but might increse risk of overfitting\n"
      ],
      "metadata": {
        "id": "ga1ThOYJ0VqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_estimators :\n",
        "number of trees in the forest\n",
        "increase in n_estimators reduce model variance and improve accuracy\n",
        "\n",
        "too many n_estimators can lead to computational cost with out singnificant improvment"
      ],
      "metadata": {
        "id": "W-cQrmbxC0LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_depth : this parameter limits depth of each tree\n",
        "less depth : less chance of overfitting | captures less data patterns\n",
        "big depth : risk of overfitting | captures more data patterns"
      ],
      "metadata": {
        "id": "bxQL6wewECWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "impact of using highly correlated to each other\n",
        "\n",
        "--> highly correlated features can mislead feature importaance scores\n",
        "-->"
      ],
      "metadata": {
        "id": "ihxZxah1EjZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out of bag error\n",
        "oob error -  average error for each calcultedd using predictions from tree\n",
        "do not contain in their respective bootstrap sample.\n",
        " The OOB error for the entire random forest is computed by averaging the OOB errors of the individual trees."
      ],
      "metadata": {
        "id": "7ja0XLKYG1-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bootstrap=True (the default setting),\n",
        "each tree is trained on a different bootstrap sample\n",
        "of the data, leading to decorrelated trees and more robust predictions.\n",
        "\n",
        "***********************\n",
        "If bootstrap=False, each tree is trained on the entire dataset,\n",
        "which increases correlation between trees,\n"
      ],
      "metadata": {
        "id": "YZ8bBZPUNHwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Handle imbalanced datasets in random forest?\n",
        "\n",
        "class weighting -\n",
        "assign higher weights to minority class to makethe more sensitive.\n",
        "synthetic data generation\n",
        "\n",
        "balanced subsampling -\n",
        "use SMOTE or undersampling of majority class to create balanced dataset.\n",
        "\n",
        "ensemble techniques -\n",
        "combine random forest with other ensemble methods like\n",
        "easyensemble /  balanced random forest\n",
        "\n"
      ],
      "metadata": {
        "id": "Em9Saz2FQ30H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GINI IMPURITY AND ENTROPY\n",
        "ARE CRITERIA USED TO EVALUATE THE QUALITY OF SPLITS IN DT WITH RANDOM FOREST\n",
        "\n",
        "GINI IMPURITY :\n",
        "MEASURES PROBABILITY OF INCORRECTLY CLASSIFYING A RANDOMLY CHOOSEN ELEMENT.\n",
        "\n",
        "Information Gain:\n",
        "measures amount of uncertainity in dataset, it tends to produce with better splits."
      ],
      "metadata": {
        "id": "ZLrUQzvfVxG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_state: ????????????????????????????\n",
        "including bootstrap sampling and feature selection\n"
      ],
      "metadata": {
        "id": "vEeA_U3yYjY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "potential drawbacks of trees in random forest model?\n",
        "n_estimators:\n",
        "computational cost\n",
        "memory usage\n",
        "marginal gains :"
      ],
      "metadata": {
        "id": "beu2zFfcbfLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate stability of random forest:\n",
        "\n",
        "\n",
        "oob -\n",
        "cross validation -\n",
        "permutation importance - evaluating stability of feature importance scores by repeating\n",
        "\n",
        "reproducibility tests - running the model multiple times with different random seeds\n",
        "seeds and comparing the results can provide insight into the stability of the model.\n"
      ],
      "metadata": {
        "id": "zpOd2PKscJ0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "8d7YIG7Ow9ae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1. What is a Support Vector Machine (SVM)?\n",
        "- classification tasks\n",
        "- regression\n",
        "- SVMs work by finding the hyperplane that best separates the data into different classes\n",
        "- The goal is to find the maximum margin hyperplane\n",
        "-"
      ],
      "metadata": {
        "id": "zCTyk0DdqgpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. Explain the concept of the hyperplane in SVM?\n",
        "\n",
        "- A hyperplane in SVM is a decision boundary that separates different classes of data points\n"
      ],
      "metadata": {
        "id": "v-x2aombs57m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. What is the \"margin\" in SVM, and why is it important?\n",
        "The margin in SVM refers to the distance between the hyperplane\n",
        " and the closest data points from either class, known as support vectors."
      ],
      "metadata": {
        "id": "H1uHWM8Vt-Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. What are support vectors in SVM?\n",
        "Support vectors are the data points that lie closest to the hyperplane and are most difficult to classify"
      ],
      "metadata": {
        "id": "siLat1giuJR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. Explain the difference between a hard margin and a soft margin in SVM.\n",
        "This approach requires that the data be perfectly linearly separable\n",
        "\n",
        "Soft Margin SVM: This approach allows for some misclassifications or violations of the margin to account for outliers and noise in the data."
      ],
      "metadata": {
        "id": "Ur7dQ0IouZi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. What is the kernel trick in SVM, and why is it used?\n",
        "The kernel trick is a technique used in SVM to transform the input data into a higher-dimensional space where it becomes easier to separate the classes linearly.\n",
        " Kernels allow SVM to create nonlinear decision boundaries while still solving the problem using efficient linear methods.\n"
      ],
      "metadata": {
        "id": "QH-KJgTgu8Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. What are the different types of kernels used in SVM?\n",
        "\n",
        "Linear kernal\n",
        "polyqnomial kernal\n",
        "rbf kernal\n",
        "sigmoid kernal\n"
      ],
      "metadata": {
        "id": "nxSTjuFyvLNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear keranal - when you have large number of feature compared to number of samples\n",
        "\n",
        "polynomial kernal - when you expect interaction between features that can be polynomial terms\n",
        "\n",
        "RBF keranal - decision boundary is non linear and you\n",
        "have no prior knowledge"
      ],
      "metadata": {
        "id": "3qH2A_lVvc_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN"
      ],
      "metadata": {
        "id": "Ebyf131hxA0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn - classification and regresiion\n",
        "k - hyper parameter\n"
      ],
      "metadata": {
        "id": "xiA8NYk32dR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1 . k?\n",
        "finding k nearest neighbors\n",
        "distance - euclidean distance, manhanttan distance, cosine distance\n"
      ],
      "metadata": {
        "id": "RkaBlA493aF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disadvantage:\n",
        "huge dataset\n",
        "outliers\n",
        "missing values\n"
      ],
      "metadata": {
        "id": "ckD6Nhxz4a9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is the K-Nearest Neighbors (KNN) algorithm? How does it work?\n",
        "K-Nearest Neighbors (KNN) is a simple, non-parametric, and lazy learning algorithm used for classification and regression. The algorithm classifies a data point based on how its neighbors are classified. It works by calculating the distance between the input sample and all samples in the training data, selecting the K nearest neighbors, and then making a prediction based on the majority class (for classification) or the average (for regression) of those neighbors."
      ],
      "metadata": {
        "id": "Wjx36EuI5A8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "How do you choose the value of K in the KNN algorithm? What are the consequences of choosing a very small or very large K?\n",
        "choose k by experiment\n",
        "small k makes algo sensitive to noise leading to overfitting\n",
        "large k might smooth out decision boundary leading to underfitting"
      ],
      "metadata": {
        "id": "q8bswFkX5jls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature scaling is must..\n",
        "if feature1 is 0 to 1\n",
        "if feature2 is o to 100\n",
        "feature with big distance will dominate\n",
        "leads to biased predictions\n",
        "solution is normalization and standardization ensures that all features have a similar scale."
      ],
      "metadata": {
        "id": "PZm_P4VQ6syp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Explain the curse of dimensionality in the context of KNN. How does it impact the algorithm's performance?\n",
        "The curse of dimensionality refers to the phenomenon where the feature space becomes increasingly sparse as the number of dimensions increases. In high-dimensional spaces, the distance between any two points becomes large and almost uniform, making it difficult for KNN to distinguish between near and far points. This can lead to poor performance because the notion of proximity, which is central to KNN, becomes less meaningful."
      ],
      "metadata": {
        "id": "LxQeRuhd7zNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KNN is often referred to as a \"lazy learner.\" What does this mean?\n",
        "KNN is called a lazy learner because it does not learn a discriminative function from the training data. Instead, it stores the training data and makes predictions only when a query is made, performing the classification or regression at that time. This is in contrast to \"eager learners,\" which build a model based on the training data before any queries are made."
      ],
      "metadata": {
        "id": "lmZ08AwC87N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Advantages:\n",
        "Simple and easy to implement.\n",
        "No assumptions about the data distribution.\n",
        "Naturally handles multi-class classification problems.\n",
        "Disadvantages:\n",
        "Computationally expensive at prediction time, especially with large datasets.\n",
        "Sensitive to the choice of K and the distance metric.\n",
        "Performance degrades with high-dimensional data (curse of dimensionality)."
      ],
      "metadata": {
        "id": "OKUXzsOA9S1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is weighted KNN, and how does it differ from the standard KNN?\n",
        "weighted knn - contributions of neighbors are weighted by their distance\n",
        "standard knn - all neighbors contribute equally\n"
      ],
      "metadata": {
        "id": "3irrgceR-DSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "How does KNN handle ties when two or more classes have the same number of nearest neighbors?\n",
        "Choose randomly: One of the tied classes is chosen at random.\n",
        "Consider a weighted KNN: Weights can be used to break the tie by giving more importance to closer neighbors."
      ],
      "metadata": {
        "id": "8hh6qjFxglcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "SMNVL2u0xC-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Naive Bayes -\n",
        "workks on bayes theorem\n",
        "it calculates probability of each class given the features and\n",
        "selects the class with the highest probability as the prediction."
      ],
      "metadata": {
        "id": "Qi8PqHy5mK7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive bayes\n",
        "- it assue all features are independent of each other given the class label\n",
        "\n",
        "- this assumption is rarely true i real world\n",
        "but algo performs well in practice"
      ],
      "metadata": {
        "id": "QCqIhICPnzKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " gaussian naive bayes -\n",
        " data: suitable for continuous data\n",
        " assumption: assume feature follows a gaussian normal distribution\n",
        " use case: spam detection,medical diagnosis\n",
        " probability calculation: for each feature it calculates probability density function\n",
        " of gaussian distribution, characterized by mean and variance of each feature.\n",
        "\n",
        "\n",
        " benoulli naive bayes -\n",
        "\n",
        " data: suitable for binary data\n",
        " assumption: assumes features are binary\n",
        " use cases: often used in text classification tasks\n",
        " probability calculation: it calculates the probaility of feature being 1 or 0 in each class."
      ],
      "metadata": {
        "id": "ufIpDL12vojR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "used for multi class classification"
      ],
      "metadata": {
        "id": "NU7eiikF3DDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "difference btwn naive bayes and logistic regression?\n",
        "\n",
        "in naive bayes assumes feature independence\n",
        "logistic regression does not make such an assumption\n",
        "\n",
        "logistic regressionis more interpretable model with coefficients\n",
        "\n",
        "data requirements:\n",
        "naive bayes can wo;rk with smaller dataset\n",
        "logistic regression requires large dataset"
      ],
      "metadata": {
        "id": "ELSHBPDC3KR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "we can deal with continious variable by using **gaussian naive bayes**"
      ],
      "metadata": {
        "id": "WheK4gKc4UK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "laplace smoothing?\n",
        "is used to handle the *zero frequency problem* by adding a small constant to the count of each feature."
      ],
      "metadata": {
        "id": "kET0N-mu92l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "limitations of naive bayes:\n",
        "\n",
        "feature independence assumption\n",
        "zero frequency problem\n"
      ],
      "metadata": {
        "id": "7NzIDu3HB2Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "advantages of Naive Bayes?\n",
        "\n",
        "NB is fast to train and predict.\n",
        "it works well with large number of feaures.\n",
        " nb is simple and efficient to implement."
      ],
      "metadata": {
        "id": "_QbnseybCqBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K - means clustering"
      ],
      "metadata": {
        "id": "NWWTK1Iexcs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k means clustering is unsupervised algo.\n",
        "\n",
        "partition dataset into k clusters based on similarity\n",
        "each cluster is represented by its centroid\n",
        "\n"
      ],
      "metadata": {
        "id": "YqvyQxzbI_W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intial k centroids\n",
        "\n",
        "assign each point to nearest centroids.\n",
        "\n",
        "calculate new centroid of each cluster based on mean of position\n"
      ],
      "metadata": {
        "id": "rqcn7GBPKO1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k is determined by elbow method\n",
        "wcss is used to determine k"
      ],
      "metadata": {
        "id": "wT_YRi5MqsgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA"
      ],
      "metadata": {
        "id": "Wjm1dDuFxg17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca is used for dimensionality reduction\n",
        "scale features before using pca\n",
        "accuracy might drop\n",
        "pca is used to reduce dimensionality\n",
        "curse of dimensionality"
      ],
      "metadata": {
        "id": "Yc48SMBwtjB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first thing is\n",
        "calculte variance\n",
        "variance is calculated by eigen vectors\n"
      ],
      "metadata": {
        "id": "cSzHHU0WvJhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if eigen vector is big which mwnas it has more variance\n",
        "if eigen vector is small which mwnas it has less variance\n",
        "\n"
      ],
      "metadata": {
        "id": "gyLncnIr3gOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "how do you reduce features using pca?\n",
        "\n",
        "lets say there are three features you going to claculate three eigen values\n",
        "by combining eigen values we can convert 3d to 2d or 3d to 1d.\n"
      ],
      "metadata": {
        "id": "Ld-BhyJC3rdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eigen values and vectors are calculated by covariance matrix\n",
        "if 2 features 2 * 2 matrix and 3 features 3 * 3 matrix\n",
        "\n",
        "covariance matrix -  it calcultes how one feature is effecting other feature"
      ],
      "metadata": {
        "id": "pdy9C9zO4EeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Each eigenvector corresponds to a principal component, and the eigenvalue\n",
        "associated with it indicates the amount of variance captured by that component\n",
        "\n"
      ],
      "metadata": {
        "id": "_O8ADeXh6SyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process:\n",
        "\n",
        "\n",
        "standardize data\n",
        "\n",
        "calculate covariance matrix\n",
        "\n",
        "find eigen values, eigen vectors\n",
        "\n",
        "sort eigen values\n",
        "\n",
        "eigen values -  highest variance of data\n",
        "\n",
        "select k eigen vectors\n",
        "\n",
        "transform data"
      ],
      "metadata": {
        "id": "09z3NVhA7Qbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosing"
      ],
      "metadata": {
        "id": "asDMDI5sx30P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boosting -\n",
        "decision tree are sequentially connected\n",
        "\n",
        "\n",
        "diagram :\n",
        "\n",
        "dataset --> dt1 --> dt2 --> dt3--> dt4-->dtn --> final prediction\n",
        "\n",
        "dt1 --> weak learner ---> haven't learned much from dataset\n",
        "dtn --> string learner -->\n"
      ],
      "metadata": {
        "id": "uFzT9N-P8IyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boosting -- classification\n",
        "-- regression\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "78Dx_tYW9m7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost classifier\n",
        "\n",
        "during adaboost, the weights of incorrectly classdified samples are increased\n",
        "so next week learners focus more on these samples.\n",
        "\n",
        "adaboost uses decision stump\n",
        "adaboost more sensitive to outliers\n",
        "\n",
        "gradient boosting classifier\n",
        "\n",
        "updates weight by negative gradient of loss function with respect to predicted op\n",
        "gradient boosting generally more robust as it\n",
        " updates weights based on gradients, which are less sensitive to outliers.\n"
      ],
      "metadata": {
        "id": "vJdzlGjV9zKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgboost is optimized version of gradient boosting classifier\n",
        "\n",
        "increases speed and performance\n",
        "\n",
        "xgboost for both classification and regression tasks."
      ],
      "metadata": {
        "id": "2lVM1IeWrHXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lightgbm is used for large dataset and large features\n"
      ],
      "metadata": {
        "id": "1neEyUn7rhhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stochastic gradient boosting\n",
        "\n",
        "is used when concern is overfitting"
      ],
      "metadata": {
        "id": "GTnNJBBbskon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histgradientboosting\n",
        "\n",
        "used to speed up training process\n",
        "HistGradientBoosting for scalable machine learning tasks."
      ],
      "metadata": {
        "id": "ZP5fvC-LsvqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagging"
      ],
      "metadata": {
        "id": "bszVVFdCx6F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data --> data splicing --> to decision trees --> aggregator"
      ],
      "metadata": {
        "id": "YoW1ooaHusiH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}